{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":643971,"sourceType":"datasetVersion","datasetId":319080}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-28T14:33:07.469383Z","iopub.execute_input":"2024-09-28T14:33:07.469816Z","iopub.status.idle":"2024-09-28T14:33:09.375930Z","shell.execute_reply.started":"2024-09-28T14:33:07.469776Z","shell.execute_reply":"2024-09-28T14:33:09.374534Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!cd ../ && cd input && cd skin-cancer9-classesisic && cd 'Skin cancer ISIC The International Skin Imaging Collaboration' && cd Train && ls","metadata":{"execution":{"iopub.status.busy":"2024-09-28T15:18:22.626914Z","iopub.execute_input":"2024-09-28T15:18:22.627357Z","iopub.status.idle":"2024-09-28T15:18:23.701501Z","shell.execute_reply.started":"2024-09-28T15:18:22.627299Z","shell.execute_reply":"2024-09-28T15:18:23.699887Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"'actinic keratosis'\t melanoma\t\t      'seborrheic keratosis'\n'basal cell carcinoma'\t nevus\t\t\t      'squamous cell carcinoma'\n dermatofibroma\t\t'pigmented benign keratosis'  'vascular lesion'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Imports\n- torch: This is the main PyTorch library used for building and training models.\n- torch.nn: Contains modules for creating neural networks.\n- torch.optim: Contains optimizers like SGD, Adam, etc., to update model weights during training.\n- torchvision: Helps with image data manipulation and model loading.\n- datasets: Provides utilities to load image datasets.\n- models: Contains pre-trained models that we can use (e.g., ResNet, VGG).\n- transforms: Used for preprocessing the images (like resizing, normalizing, etc.).\n- DataLoader: Used to load batches of data in a fast, efficient manner.\n- os: Helps navigate directories.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import random_split, DataLoader\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-09-28T18:37:14.730430Z","iopub.execute_input":"2024-09-28T18:37:14.730718Z","iopub.status.idle":"2024-09-28T18:37:19.972766Z","shell.execute_reply.started":"2024-09-28T18:37:14.730685Z","shell.execute_reply":"2024-09-28T18:37:19.971868Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Data Transformations(for both Training and Testing Images) and Augmentation(Only for Training Images)","metadata":{}},{"cell_type":"code","source":"data_transforms = {\n    # Training Transformations\n    'train': transforms.Compose([\n        transforms.Resize(256), # Resize the shorter side of image to 256 Pixels while preserving the Aspect Ratio\n        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)), # Randomly crop the image to 224*224, ensuring we keep at least 80% of the original image\n        transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally for Data Augmentation\n        transforms.ToTensor(), # Convert Image to PyTorch Tensor\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalize the Image with Mean and Standard Deviation for each channel (ImageNet pre-trained Models expect this)\n    ]),\n    \n    # Validation Transformation\n    'val': transforms.Compose([\n        transforms.Resize(256), # Resize the Shorter Side of the Image to 256 pixels\n        transforms.CenterCrop(224),\n        transforms.ToTensor(), # Convert Image to PyTorch Tensor\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalize the image(same normalization as in training to match the pre-trained model)\n    ])\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-28T18:54:48.330752Z","iopub.execute_input":"2024-09-28T18:54:48.331497Z","iopub.status.idle":"2024-09-28T18:54:48.338138Z","shell.execute_reply.started":"2024-09-28T18:54:48.331454Z","shell.execute_reply":"2024-09-28T18:54:48.337238Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Set up Datasets and Data Loaders for Training and Validation","metadata":{}},{"cell_type":"code","source":"# Define the directory path for training and validation datasets\ndata_dir = '../input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration'\n\n# Load datasets with ImageFolder, which expects data to be organized by class folders\n\"\"\"\nImageFolder function automatically labels images based on their folder names (e.g., melanoma, nevus, etc.).\nIt loads images from the directory and applies the transformations defined earlier.\n\"\"\"\ntrain_dataset_full = datasets.ImageFolder(os.path.join(data_dir, 'Train'), data_transforms['train'])\n\n# Split the train dataset into training and validation sets\ntrain_ratio = 0.8\ntrain_size = int(train_ratio * len(train_dataset_full))\nval_size = len(train_dataset_full) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset_full, [train_size, val_size])\n\ntest_dataset = datasets.ImageFolder(os.path.join(data_dir, 'Test'), data_transforms['val'])\n\n# image_datasets = {\n#     'train': datasets.ImageFolder(os.path.join(data_dir,'Train'), data_transforms['train']), # Load and pre-process the Train data\n#     'val': datasets.ImageFolder(os.path.join(data_dir, 'Test'), data_transforms['val']) # # Load and pre-process the Test data\n# }\n\n# Create Dataloaders to load the data in batches of 32\ndataloaders = {\n    'train': DataLoader(train_dataset, batch_size = 32, shuffle = True, num_workers = 4), # Shuffle is done to ensure Randomness in sequence of data\n    'val': DataLoader(val_dataset, batch_size = 32, shuffle = False, num_workers = 4),\n    'test': DataLoader(test_dataset, batch_size = 32, shuffle = False, num_workers = 4)\n}\n\n# Get the sizes of the datasets for future reference\ndataset_sizes = {\n    'train': len(train_dataset),\n    'val': len(val_dataset),\n    'test': len(test_dataset)\n}\n# dataset_sizes = {x : len(image_datasets[x]) for x in ['train', 'val']} # Returns dictionary of form {'train':1000, 'val':200}\n\n\nclass_names = train_dataset_full.classes # Get the class names (directories names) for label mapping\n\n# Check if GPU is available and set the device accordingly\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Print important insights\nprint(f\"The size of Train Image = {dataset_sizes['train']}\")\nprint(f\"The size of Test Image = {dataset_sizes['val']}\")\nprint(f\"The device used => {device}\")\nprint(f\"The class names are: {class_names}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T18:55:53.514944Z","iopub.execute_input":"2024-09-28T18:55:53.515646Z","iopub.status.idle":"2024-09-28T18:55:53.553517Z","shell.execute_reply.started":"2024-09-28T18:55:53.515604Z","shell.execute_reply":"2024-09-28T18:55:53.552640Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"The size of Train Image = 1791\nThe size of Test Image = 448\nThe device used => cuda:0\nThe class names are: ['actinic keratosis', 'basal cell carcinoma', 'dermatofibroma', 'melanoma', 'nevus', 'pigmented benign keratosis', 'seborrheic keratosis', 'squamous cell carcinoma', 'vascular lesion']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Initialize a Pre-Trained Model for Transfer Learning","metadata":{}},{"cell_type":"code","source":"model_ft = models.resnet18(weights = models.ResNet18_Weights.DEFAULT) # Load the Pre-trained ResNet18 Model\n\"\"\"\nResNet18 Architecture Before: Convolutional Layers -> Global Average Pooling -> FC (512 -> 1000)\n\"\"\"\nnum_ftrs = model_ft.fc.in_features # Get the number of input features to the final fully connected layer\n\nmodel_ft.fc = nn.Linear(num_ftrs, len(class_names)) # Modify the final layer to match the number of classes (9 in our case)\n\"\"\"\nResNet18 Architecture Before: Convolutional Layers -> Global Average Pooling -> FC (512 -> 9)\n\"\"\"\nmodel_ft = model_ft.to(device) # Move the model the the device (GPU if available otherwise to the CPU)\n\ncriterion = nn.CrossEntropyLoss() # Defining the Loss function (Cross Entropy Loss for multi-class Classification)\n\n# Freeze the earlier layers and only fine-tune the fully connected layers\nfor param in model_ft.parameters():\n#     param.requires_grad = False # Freeze all layers except the last layer\n      param.requires_grad = True # Allow Updating of Convolution and Pooling layers because Freezing did not give good results\n\n# Unfreeze the fully connected layer (the one we modified)\nfor param in model_ft.fc.parameters():\n    param.requires_grad = True # Allow updating of the Final Layer only\n\noptimizer_ft = optim.Adam(model_ft.parameters(), lr = 1e-4) # Setting up the Optimizer (Adam in this case)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T18:57:42.635895Z","iopub.execute_input":"2024-09-28T18:57:42.636661Z","iopub.status.idle":"2024-09-28T18:57:42.895234Z","shell.execute_reply.started":"2024-09-28T18:57:42.636611Z","shell.execute_reply":"2024-09-28T18:57:42.894379Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Training and Validating the Model","metadata":{}},{"cell_type":"code","source":"# Function to train the Model\ndef train_model(model, dataloaders, criterion, optimizer, num_epochs = 25):\n    # Track the best model based on Validation Accuracy\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n    \n    # Loop over the epochs\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(\"-\"*10)\n        \n        # Each epoch has a training and validation loop\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train() # Set the Model to training mode\n            else:\n                model.eval() # Set the Model to Evaluation mode\n            \n            running_loss = 0.0\n            running_corrects = 0\n            \n            # Iterate over data batches\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                # Zero the parameter gradients\n                optimizer.zero_grad() # Clear the gradients from the previous batch to avoid accumulation.\n                \n                # Forward Pass: Track history if only in Train\n                with torch.set_grad_enabled(phase == 'train'): # torch.set_grad_enabled(True) means keep track of Gradients\n                    outputs = model(inputs) # Perform a forward pass through the model.\n                    _, preds = torch.max(outputs, 1) # Returns max_value and index_of_max_value among the second dimension(1) of the (32,9) logits outputs\n                    loss = criterion(outputs, labels)\n                    \n                    # Backward pass and optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward() # Computes the gradients using backpropagation\n                        optimizer.step() # Updates the model parameters based on the gradients\n                \n                # Track the Running loss and Number of correct predictions for each batch\n                running_loss += loss.item() * inputs.size(0) # loss.item() gives the average loss over the batch in scalar form rather than Tensor(thanks to .item() function) which is then multiplied by the batch size (inputs.size(0)) to obtain the total loss in the batch\n                running_corrects += torch.sum(preds == labels.data)\n                \n            # Calculate epoch loss and acuracy\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            \n            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n            \n            # Deep copy the model if validation accuracy is the best\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = model.state_dict()\n    \n    # Load the best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T18:57:46.323933Z","iopub.execute_input":"2024-09-28T18:57:46.324597Z","iopub.status.idle":"2024-09-28T18:57:46.336440Z","shell.execute_reply.started":"2024-09-28T18:57:46.324557Z","shell.execute_reply":"2024-09-28T18:57:46.335441Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Train the Model","metadata":{}},{"cell_type":"code","source":"model_ft = train_model(model_ft, dataloaders, criterion, optimizer_ft, num_epochs = 25)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T18:57:46.789295Z","iopub.execute_input":"2024-09-28T18:57:46.790134Z","iopub.status.idle":"2024-09-28T19:04:41.455940Z","shell.execute_reply.started":"2024-09-28T18:57:46.790091Z","shell.execute_reply":"2024-09-28T19:04:41.454871Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 1/25\n----------\ntrain Loss: 1.3718 Acc: 0.5366\nval Loss: 1.0352 Acc: 0.6138\nEpoch 2/25\n----------\ntrain Loss: 0.7395 Acc: 0.7443\nval Loss: 0.8370 Acc: 0.7143\nEpoch 3/25\n----------\ntrain Loss: 0.5692 Acc: 0.7873\nval Loss: 0.7951 Acc: 0.7232\nEpoch 4/25\n----------\ntrain Loss: 0.4338 Acc: 0.8481\nval Loss: 0.7725 Acc: 0.7031\nEpoch 5/25\n----------\ntrain Loss: 0.3480 Acc: 0.8688\nval Loss: 0.7370 Acc: 0.7277\nEpoch 6/25\n----------\ntrain Loss: 0.2760 Acc: 0.8978\nval Loss: 0.8091 Acc: 0.7143\nEpoch 7/25\n----------\ntrain Loss: 0.2466 Acc: 0.9056\nval Loss: 0.7976 Acc: 0.7143\nEpoch 8/25\n----------\ntrain Loss: 0.2034 Acc: 0.9196\nval Loss: 0.7149 Acc: 0.7366\nEpoch 9/25\n----------\ntrain Loss: 0.2038 Acc: 0.9090\nval Loss: 0.8637 Acc: 0.7054\nEpoch 10/25\n----------\ntrain Loss: 0.2107 Acc: 0.9107\nval Loss: 0.8241 Acc: 0.6920\nEpoch 11/25\n----------\ntrain Loss: 0.1792 Acc: 0.9179\nval Loss: 0.7720 Acc: 0.7277\nEpoch 12/25\n----------\ntrain Loss: 0.1733 Acc: 0.9202\nval Loss: 0.7829 Acc: 0.7321\nEpoch 13/25\n----------\ntrain Loss: 0.1622 Acc: 0.9179\nval Loss: 0.7943 Acc: 0.7277\nEpoch 14/25\n----------\ntrain Loss: 0.1605 Acc: 0.9190\nval Loss: 0.9186 Acc: 0.6942\nEpoch 15/25\n----------\ntrain Loss: 0.1607 Acc: 0.9229\nval Loss: 0.8887 Acc: 0.7098\nEpoch 16/25\n----------\ntrain Loss: 0.1465 Acc: 0.9291\nval Loss: 0.9218 Acc: 0.6920\nEpoch 17/25\n----------\ntrain Loss: 0.1466 Acc: 0.9202\nval Loss: 0.9284 Acc: 0.7009\nEpoch 18/25\n----------\ntrain Loss: 0.1399 Acc: 0.9252\nval Loss: 0.8442 Acc: 0.7210\nEpoch 19/25\n----------\ntrain Loss: 0.1283 Acc: 0.9296\nval Loss: 0.8226 Acc: 0.7344\nEpoch 20/25\n----------\ntrain Loss: 0.1248 Acc: 0.9263\nval Loss: 0.9457 Acc: 0.6987\nEpoch 21/25\n----------\ntrain Loss: 0.1253 Acc: 0.9363\nval Loss: 0.9938 Acc: 0.7277\nEpoch 22/25\n----------\ntrain Loss: 0.1247 Acc: 0.9363\nval Loss: 0.8084 Acc: 0.7567\nEpoch 23/25\n----------\ntrain Loss: 0.1121 Acc: 0.9302\nval Loss: 0.9063 Acc: 0.7299\nEpoch 24/25\n----------\ntrain Loss: 0.1134 Acc: 0.9397\nval Loss: 1.0136 Acc: 0.7188\nEpoch 25/25\n----------\ntrain Loss: 0.1072 Acc: 0.9430\nval Loss: 0.9394 Acc: 0.7344\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluate the Model in Test Dataset","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, dataloader, criterion):\n    model.eval()  # Set the model to evaluation mode\n    running_loss = 0.0\n    running_corrects = 0\n\n    # Turn off gradients for faster computation\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n\n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n    # Calculate total loss and accuracy\n    total_loss = running_loss / len(dataloader.dataset)\n    total_acc = running_corrects.double() / len(dataloader.dataset)\n\n    print(f'Test Loss: {total_loss:.4f} Acc: {total_acc:.4f}')\n    return total_loss, total_acc\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T19:04:41.458471Z","iopub.execute_input":"2024-09-28T19:04:41.459326Z","iopub.status.idle":"2024-09-28T19:04:41.467017Z","shell.execute_reply.started":"2024-09-28T19:04:41.459285Z","shell.execute_reply":"2024-09-28T19:04:41.466046Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc = evaluate_model(model_ft, dataloaders['test'], criterion)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T19:05:42.057816Z","iopub.execute_input":"2024-09-28T19:05:42.058747Z","iopub.status.idle":"2024-09-28T19:05:46.138492Z","shell.execute_reply.started":"2024-09-28T19:05:42.058690Z","shell.execute_reply":"2024-09-28T19:05:46.137281Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Test Loss: 2.6205 Acc: 0.4915\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Save the Trained Model","metadata":{}},{"cell_type":"code","source":"torch.save(model_ft.state_dict(), 'best_model_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T19:07:38.177190Z","iopub.execute_input":"2024-09-28T19:07:38.177884Z","iopub.status.idle":"2024-09-28T19:07:38.256562Z","shell.execute_reply.started":"2024-09-28T19:07:38.177839Z","shell.execute_reply":"2024-09-28T19:07:38.255745Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Load the Saved Model","metadata":{}},{"cell_type":"code","source":"model_ft.load_state_dict(torch.load('best_model_weights.pth', weights_only = True))\n\nmodel_ft = model_ft.to(device)\n\nmodel_ft.eval()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T19:07:40.290955Z","iopub.execute_input":"2024-09-28T19:07:40.291717Z","iopub.status.idle":"2024-09-28T19:07:40.367326Z","shell.execute_reply.started":"2024-09-28T19:07:40.291674Z","shell.execute_reply":"2024-09-28T19:07:40.366422Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=9, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Analyze the Model Performance","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Assuming you have your predictions and true labels stored in arrays\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for inputs, labels in dataloaders['test']:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        outputs = model_ft(inputs)\n        _, preds = torch.max(outputs, 1)\n        all_preds.append(preds.cpu().numpy())\n        all_labels.append(labels.cpu().numpy())\n\n# Flatten the lists of predictions and labels\nall_preds = np.concatenate(all_preds)\nall_labels = np.concatenate(all_labels)\n\n# Compute confusion matrix\nconf_matrix = confusion_matrix(all_labels, all_preds)\n\nprint(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T19:07:49.412208Z","iopub.execute_input":"2024-09-28T19:07:49.412702Z","iopub.status.idle":"2024-09-28T19:08:34.269630Z","shell.execute_reply.started":"2024-09-28T19:07:49.412663Z","shell.execute_reply":"2024-09-28T19:08:34.268383Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"[[ 1  0  0  0 12  3  0  0  0]\n [ 0 14  0  0  0  2  0  0  0]\n [ 0  2  6  1  2  5  0  0  0]\n [ 0  0  0  1 11  3  0  0  1]\n [ 0  0  0  1 15  0  0  0  0]\n [ 0  1  0  0  0 15  0  0  0]\n [ 0  0  0  3  0  0  0  0  0]\n [ 0  2  0  2  1  8  0  3  0]\n [ 0  0  0  0  0  0  0  0  3]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Calculate accuracy, precision, recall, and F1-score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-09-28T19:09:33.764020Z","iopub.execute_input":"2024-09-28T19:09:33.764433Z","iopub.status.idle":"2024-09-28T19:09:33.768969Z","shell.execute_reply.started":"2024-09-28T19:09:33.764393Z","shell.execute_reply":"2024-09-28T19:09:33.768057Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T19:12:36.453569Z","iopub.execute_input":"2024-09-28T19:12:36.453948Z","iopub.status.idle":"2024-09-28T19:12:36.470974Z","shell.execute_reply.started":"2024-09-28T19:12:36.453911Z","shell.execute_reply":"2024-09-28T19:12:36.469914Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Accuracy: 0.4915\nPrecision: 0.6742\nRecall: 0.4915\nF1-Score: 0.4239\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}